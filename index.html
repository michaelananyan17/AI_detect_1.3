<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI/Human Text Detector Pipeline - Live Prediction Only</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.x/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .container { display: flex; flex-wrap: wrap; gap: 20px; }
        .section { border: 1px solid #ccc; padding: 15px; border-radius: 5px; flex: 1 1 300px; }
        h3 { border-bottom: 2px solid #eee; padding-bottom: 5px; }
        .status { margin-top: 10px; font-weight: bold; }
        #live-prediction-output { margin-top: 15px; padding: 10px; border-radius: 5px; min-height: 50px; }
        .human { background-color: #e6ffe6; color: green; }
        .ai { background-color: #ffe6e6; color: red; }
    </style>
</head>
<body>

    <h1>AI/Human Text Detection Pipeline (Simplified to Live Prediction)</h1>
    <div class="container">

        <div class="section" id="step1">
            <h3>1. DATA LOAD & PREPARATION</h3>
            <label>Train CSV:</label>
            <input type="file" id="trainFile" accept=".csv"><br><br>
            <label>Test CSV (Needed for data parsing, though not used for final steps):</label>
            <input type="file" id="testFile" accept=".csv"><br><br>
            <button onclick="loadAndPrepareData()">Load & Prepare Data</button>
            <div class="status" id="loadStatus">Awaiting File Upload...</div>
        </div>

        <div class="section" id="step3">
            <h3>3. PREPROCESSING & TOKENIZATION</h3>
            <label>Max Sequence Length:</label>
            <input type="number" id="maxLength" value="150" min="10">
            <label>Embedding Dimension:</label>
            <input type="number" id="embeddingDim" value="128" min="32"><br><br>
            <button onclick="preprocessAndTokenize()" disabled id="tokenizeBtn">Tokenize & Split</button>
            <div class="status" id="tokenizeStatus"></div>
        </div>

        <div class="section" id="step56">
            <h3>5 & 6. MODEL CONSTRUCTION & TRAINING</h3>
            <button onclick="trainModel()" disabled id="trainBtn">Build & Train Model</button>
            <div class="status" id="trainStatus"></div>
        </div>

        <div class="section" id="step7">
            <h3>7. LIVE PREDICTION INTERFACE</h3>
            <textarea id="liveTextInput" rows="5" cols="40" placeholder="Paste the ANSWER TEXT only here..."></textarea><br>
            <button onclick="livePredict()" disabled id="predictBtn">Predict</button>
            <div id="live-prediction-output"></div>
        </div>
        
    </div>
    
    <div id="vis-container"></div>

    <script>
        // Global variables to hold data, model, and vocabulary
        let trainData, testData;
        let trainXs, trainYs, valXs, valYs, testXs; // Tensors kept for training/tokenization only
        let wordIndex = {};
        let vocabSize = 0;
        let model;
        
        // Helper to safely dispose of a tensor if it exists
        function safeDispose(tensor) {
            if (tensor && tensor.isDisposed === false) {
                tensor.dispose();
                return true;
            }
            return false;
        }

        // --- STEP 1: DATA LOAD & PREPARATION ---
        async function loadAndPrepareData() {
            const trainFile = document.getElementById('trainFile').files[0];
            const testFile = document.getElementById('testFile').files[0];
            const loadStatus = document.getElementById('loadStatus');

            if (!trainFile || !testFile) {
                loadStatus.textContent = "Please upload both train.csv and test.csv.";
                return;
            }

            try {
                // Helper to read and parse CSV
                const parseCSV = (file) => new Promise((resolve, reject) => {
                    const reader = new FileReader();
                    reader.onload = (e) => {
                        const csvText = e.target.result;
                        const lines = csvText.split('\n').filter(l => l.trim() !== '');
                        const header = lines[0].split(',').map(h => h.replace(/^"|"$/g, '').trim());
                        
                        const textColIndex = header.indexOf('answer_text');
                        const labelColIndex = header.indexOf('label');

                        if (textColIndex === -1 || labelColIndex === -1) {
                            return reject("CSV must contain 'answer_text' and 'label' columns.");
                        }

                        const data = lines.slice(1).map(line => {
                            const parts = line.split(/,(?=(?:(?:[^"]*"){2})*[^"]*$)/); 
                            
                            if (parts.length < header.length) return null; 

                            let textPart = parts[textColIndex];
                            let labelPart = parts[labelColIndex];
                            
                            if (typeof textPart === 'undefined' || typeof labelPart === 'undefined') return null;

                            let text = textPart.replace(/^"|"$/g, '').replace(/\\n/g, ' ').replace(/\\r/g, ' ');
                            let label = parseInt(labelPart);

                            if (!isNaN(label) && (label === 0 || label === 1) && text.length > 0) {
                                return { text: text, label: label, original_text: textPart };
                            }
                            return null;
                        }).filter(entry => entry !== null); 

                        resolve(data);
                    };
                    reader.onerror = reject;
                    reader.readAsText(file);
                });

                loadStatus.textContent = "Processing data...";
                trainData = await parseCSV(trainFile);
                testData = await parseCSV(testFile); // Keep for parsing symmetry

                loadStatus.textContent = `✅ Data Loaded! Train samples: ${trainData.length}, Test samples: ${testData.length}.`;
                document.getElementById('tokenizeBtn').disabled = false;
                
                // --- STEP 2: DATA INSPECTION & ANALYSIS ---
                analyzeData(trainData);

            } catch (error) {
                loadStatus.textContent = `❌ Error loading data: ${error}`;
                console.error(error);
            } finally {
                tf.tidy(() => {}); 
            }
        }

        // --- STEP 2: DATA INSPECTION & ANALYSIS (Visualization) ---
        function analyzeData(data) {
            const labels = data.map(d => d.label);
            const humanCount = labels.filter(l => l === 0).length;
            const aiCount = labels.filter(l => l === 1).length;
            const total = labels.length;

            const values = [
                { index: 'Human (0)', value: humanCount },
                { index: 'AI (1)', value: aiCount }
            ];

            const container = document.getElementById('vis-container');
            container.innerHTML = ''; 
            tfvis.render.barchart(
                container,
                values,
                {
                    title: `Class Balance (Total: ${total} samples)`,
                    height: 250,
                    fontSize: 14
                }
            );

            const imbalanceRatio = Math.min(humanCount, aiCount) / Math.max(humanCount, aiCount);
            if (imbalanceRatio < 0.5) {
                const flag = document.createElement('div');
                flag.textContent = "⚠️ Warning: Dataset is significantly imbalanced (ratio < 0.5).";
                flag.style.color = 'orange';
                container.appendChild(flag);
            }
        }

        // --- STEP 3: TEXT PREPROCESSING & TOKENIZATION ---
        function preprocessAndTokenize() {
            const maxLength = parseInt(document.getElementById('maxLength').value) || 150;
            const tokenizeStatus = document.getElementById('tokenizeStatus');

            if (!trainData || trainData.length === 0) {
                tokenizeStatus.textContent = "Please load data first.";
                return;
            }

            tokenizeStatus.textContent = "Building vocabulary and tokenizing...";
            
            tf.tidy(() => { 
                // 1. Create vocabulary 
                const allTrainWords = trainData.flatMap(d => d.text.toLowerCase().split(/\W+/).filter(w => w.length > 0));
                const wordCounts = allTrainWords.reduce((acc, word) => {
                    acc[word] = (acc[word] || 0) + 1;
                    return acc;
                }, {});

                const sortedWords = Object.keys(wordCounts).sort((a, b) => wordCounts[b] - wordCounts[a]);
                
                wordIndex = {'<pad>': 0, '<unk>': 1};
                sortedWords.forEach((word, i) => {
                    wordIndex[word] = i + 2;
                });
                vocabSize = Object.keys(wordIndex).length;

                // 2. Convert text to integer sequences
                const textsToSequences = (data) => data.map(d => {
                    const sequence = d.text.toLowerCase().split(/\W+/)
                        .filter(w => w.length > 0)
                        .map(word => wordIndex[word] || wordIndex['<unk>']);

                    if (sequence.length > maxLength) {
                        return sequence.slice(0, maxLength); 
                    } else if (sequence.length < maxLength) {
                        const padding = new Array(maxLength - sequence.length).fill(wordIndex['<pad>']);
                        return sequence.concat(padding); 
                    }
                    return sequence;
                });
                
                const trainSequences = textsToSequences(trainData);

                const trainLabels = tf.tensor2d(trainData.map(d => [d.label]), [trainData.length, 1], 'float32');
                
                // 3. 80/20 train-validation split (Validation data needed for training callback)
                const trainSize = Math.floor(trainSequences.length * 0.8);

                // Dispose previous tensors safely
                safeDispose(trainXs);
                safeDispose(trainYs);
                safeDispose(valXs);
                safeDispose(valYs);
                safeDispose(testXs); // Dispose of unused test data tensor if it was created

                // Create new tensors (available globally)
                trainXs = tf.tensor2d(trainSequences.slice(0, trainSize), [trainSize, maxLength], 'int32');
                trainYs = trainLabels.slice(0, trainSize);
                valXs = tf.tensor2d(trainSequences.slice(trainSize), [trainSequences.length - trainSize, maxLength], 'int32');
                valYs = trainLabels.slice(trainSize);
                
                trainLabels.dispose();
            });


            tokenizeStatus.textContent = `✅ Tokenization Complete! Vocab Size: ${vocabSize}. 
                Train Samples: ${trainXs.shape[0]}, Validation Samples: ${valXs.shape[0]}.`;
            document.getElementById('trainBtn').disabled = false;
            document.getElementById('predictBtn').disabled = false;
        }

        // --- STEPS 4 & 5: OPTIMIZED MODEL CONSTRUCTION ---
        function createModel() {
            const embeddingDim = parseInt(document.getElementById('embeddingDim').value) || 128; 
            const maxLength = parseInt(document.getElementById('maxLength').value);
            
            if (model) model.dispose();

            model = tf.sequential();
            
            model.add(tf.layers.embedding({
                inputDim: vocabSize, 
                outputDim: embeddingDim, 
                inputLength: maxLength,
                name: 'Embedding_Layer'
            }));
            
            model.add(tf.layers.conv1d({
                filters: 128, 
                kernelSize: 5, 
                activation: 'relu',
                kernelInitializer: 'heNormal', 
                name: 'Conv1D_1'
            }));
            
            model.add(tf.layers.conv1d({
                filters: 64, 
                kernelSize: 3, 
                activation: 'relu',
                name: 'Conv1D_2'
            }));

            model.add(tf.layers.globalAveragePooling1d({
                name: 'GlobalAveragePooling'
            }));

            model.add(tf.layers.dense({ 
                units: 32, 
                activation: 'relu',
                name: 'Dense_Hidden'
            }));

            model.add(tf.layers.dropout({ 
                rate: 0.5,
                name: 'Dropout'
            }));

            model.add(tf.layers.dense({ 
                units: 1, 
                activation: 'sigmoid',
                name: 'Output_Sigmoid'
            }));

            model.compile({
                optimizer: tf.train.adam(0.0005), 
                loss: 'binaryCrossentropy',
                metrics: ['accuracy']
            });

            return model;
        }

        // --- STEP 6: MODEL TRAINING ---
        async function trainModel() {
            if (!trainXs) {
                document.getElementById('trainStatus').textContent = "Please tokenize data first.";
                return;
            }

            const trainStatus = document.getElementById('trainStatus');
            const createdModel = createModel();
            
            tfvis.show.modelSummary({ name: 'Model Summary' }, createdModel);

            trainStatus.textContent = "Training model...";
            
            // Validation data is used for monitoring training performance only
            const validationData = (valXs && valYs) ? [valXs, valYs] : undefined;
            
            const history = await createdModel.fit(trainXs, trainYs, {
                epochs: 15, 
                batchSize: 64, 
                validationData: validationData,
                callbacks: tfvis.show.fitCallbacks(
                    { name: 'Training Performance' },
                    ['loss', 'val_loss', 'acc', 'val_acc'],
                    { height: 200, callbacks: ['onEpochEnd'] }
                )
            });

            trainStatus.textContent = "✅ Training Complete! Ready for Live Prediction.";
            
            // --- Memory Management: Dispose of training and validation tensors after training ---
            safeDispose(trainXs);
            safeDispose(trainYs);
            safeDispose(valXs);
            safeDispose(valYs);
            
            // Live predict button is already enabled after tokenization, ensure the model is ready.
            document.getElementById('predictBtn').disabled = false;
        }

        // --- STEP 7: LIVE PREDICTION INTERFACE ---
        function livePredict() {
            if (!model) {
                document.getElementById('live-prediction-output').textContent = "Model not trained yet.";
                return;
            }

            const textInput = document.getElementById('liveTextInput').value;
            const outputDiv = document.getElementById('live-prediction-output');
            const maxLength = parseInt(document.getElementById('maxLength').value);

            if (!textInput.trim()) {
                outputDiv.textContent = "Please enter text to predict.";
                return;
            }

            tf.tidy(() => {
                const sequence = textInput.toLowerCase().split(/\W+/)
                    .filter(w => w.length > 0)
                    .map(word => wordIndex[word] || wordIndex['<unk>']);

                const paddedSequence = (() => {
                    if (sequence.length > maxLength) {
                        return sequence.slice(0, maxLength);
                    } else if (sequence.length < maxLength) {
                        const padding = new Array(maxLength - sequence.length).fill(wordIndex['<pad>']);
                        return sequence.concat(padding);
                    }
                    return sequence;
                })();

                const inputTensor = tf.tensor2d([paddedSequence], [1, maxLength], 'int32');

                const prediction = model.predict(inputTensor);
                const score = prediction.dataSync()[0]; 

                // Prediction logic: score < 0.5 is AI (Label 1)
                const isAI = score < 0.5; 

                const resultClass = isAI ? 'ai' : 'human';
                const resultText = isAI ? 'AI' : 'Human';
                
                // Confidence: The probability of the predicted class
                const confidence = isAI ? (1 - score) : score; 

                outputDiv.className = resultClass;
                outputDiv.innerHTML = `
                    Prediction: <span style="font-weight: bold;">${resultText}</span><br>
                    Confidence: ${Math.round(confidence * 10000) / 100}%
                `;
            });
        }
    </script>

</body>
</html>
